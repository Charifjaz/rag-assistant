import argparse
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings
from app.utils import load_api_key
from app import config


def ask_question(
    question: str,
    model_name: str,
    temperature: float,
    k: int,
    persist_path: str = "vectorstore",
    user_api_key: str = None  # üëà AJOUT ICI
) -> None:
    """
    Pose une question √† un LLM avec un contexte documentaire vectoris√© (RAG).

    Args:
        question (str): La question pos√©e par l'utilisateur.
        model_name (str): Le nom du mod√®le OpenAI √† utiliser.
        temperature (float): Temp√©rature de g√©n√©ration (plus √©lev√©e = plus cr√©atif).
        k (int): Nombre de documents √† r√©cup√©rer via recherche vectorielle.
        persist_path (str): Chemin vers l'index FAISS enregistr√©.

    Returns:
        None
    """
    # Chargement de la cl√© API depuis .env
    load_api_key(user_api_key=user_api_key)

    # Chargement de l'index vectoriel (base documentaire)
    embeddings = OpenAIEmbeddings()
    vectordb = FAISS.load_local(
        persist_path, embeddings, allow_dangerous_deserialization=True
    )  # üî• Obligatoire depuis LangChain 0.1+)

    # Initialisation du mod√®le LLM OpenAI
    llm = ChatOpenAI(model_name=model_name, temperature=temperature)

    # Construction de la cha√Æne RAG (retrieval + question)
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectordb.as_retriever(search_kwargs={"k": k}),
        return_source_documents=True,
    )

    # Ex√©cution de la requ√™te
    result = qa_chain.invoke({"query": question})

    # Affichage du r√©sultat
    print("üß† R√©ponse :\n", result["result"])
    print("\nüìÑ Sources :")
    for doc in result["source_documents"]:
        print("-", doc.metadata.get("source", "Sans source"))

    # ‚úÖ Retourne la r√©ponse (pour l‚ÄôAPI ou test)
    return {
        "result": result["result"],
        "source_documents": result["source_documents"]
    }

if __name__ == "__main__":
    # Initialisation du parser de la ligne de commande
    parser = argparse.ArgumentParser(
        description="Poser une question √† ton assistant RAG bas√© sur OpenAI"
    )

    parser.add_argument(
        "--model",
        type=str,
        default=config.DEFAULT_MODEL,
        help="Nom du mod√®le OpenAI (ex: gpt-3.5-turbo, gpt-4)",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=config.DEFAULT_TEMPERATURE,
        help="Temp√©rature du mod√®le (0.0 = d√©terministe, 1.0 = cr√©atif)",
    )
    parser.add_argument(
        "--k",
        type=int,
        default=config.DEFAULT_K,
        help="Nombre de documents pertinents √† r√©cup√©rer",
    )
    parser.add_argument(
        "--question",
        type=str,
        required=False,
        help="Question √† poser (sinon pos√©e en interactif)",
    )

    args = parser.parse_args()

    # R√©cup√©ration de la question (via argument ou input)
    if args.question:
        question = args.question
    else:
        question = input("‚ùì Pose ta question : ")

    # Ex√©cution principale
    ask_question(
        question=question, model_name=args.model, temperature=args.temperature, k=args.k
    )

